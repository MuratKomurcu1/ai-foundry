{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udd01 Deep Learning Forward & Backward Pass Demo\n", "\n", "Bu notebook, temel bir sinir a\u011f\u0131 \u00fczerinden ileri y\u00f6nl\u00fc ge\u00e7i\u015f (forward pass) ve geri yay\u0131l\u0131m (backward pass) mant\u0131\u011f\u0131n\u0131 anlat\u0131r.\n", "\n", "## Konular:\n", "- XOR problemi\n", "- Sigmoid aktivasyon fonksiyonu\n", "- Elle forward ve backward pass\n", "- MSE Loss grafi\u011fi ile e\u011fitim s\u00fcreci izleme\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# XOR veri seti\n", "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n", "y = np.array([[0],[1],[1],[0]])\n", "\n", "# A\u011f\u0131rl\u0131klar\u0131 ba\u015flat\n", "np.random.seed(42)\n", "W1 = np.random.randn(2, 4)\n", "b1 = np.zeros((1, 4))\n", "W2 = np.random.randn(4, 1)\n", "b2 = np.zeros((1, 1))\n", "\n", "# Aktivasyon fonksiyonlar\u0131\n", "def sigmoid(x):\n", "    return 1 / (1 + np.exp(-x))\n", "\n", "def sigmoid_derivative(x):\n", "    return sigmoid(x) * (1 - sigmoid(x))\n", "\n", "# E\u011fitim parametreleri\n", "learning_rate = 0.1\n", "epochs = 10000\n", "loss_history = []\n", "\n", "# E\u011fitim d\u00f6ng\u00fcs\u00fc\n", "for epoch in range(epochs):\n", "    z1 = X.dot(W1) + b1\n", "    a1 = sigmoid(z1)\n", "    z2 = a1.dot(W2) + b2\n", "    a2 = sigmoid(z2)\n", "\n", "    loss = np.mean((y - a2) ** 2)\n", "    loss_history.append(loss)\n", "\n", "    d_loss_a2 = 2 * (a2 - y)\n", "    d_a2_z2 = sigmoid_derivative(z2)\n", "    d_z2_W2 = a1\n", "\n", "    d_loss_W2 = d_z2_W2.T.dot(d_loss_a2 * d_a2_z2)\n", "    d_loss_b2 = np.sum(d_loss_a2 * d_a2_z2, axis=0, keepdims=True)\n", "\n", "    d_loss_a1 = (d_loss_a2 * d_a2_z2).dot(W2.T)\n", "    d_a1_z1 = sigmoid_derivative(z1)\n", "    d_z1_W1 = X\n", "\n", "    d_loss_W1 = d_z1_W1.T.dot(d_loss_a1 * d_a1_z1)\n", "    d_loss_b1 = np.sum(d_loss_a1 * d_a1_z1, axis=0, keepdims=True)\n", "\n", "    W2 -= learning_rate * d_loss_W2\n", "    b2 -= learning_rate * d_loss_b2\n", "    W1 -= learning_rate * d_loss_W1\n", "    b1 -= learning_rate * d_loss_b1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Loss grafi\u011fi \u00e7izimi\n", "plt.figure(figsize=(8, 4))\n", "plt.plot(loss_history)\n", "plt.title(\"E\u011fitim S\u00fcrecinde Loss (MSE)\")\n", "plt.xlabel(\"Epoch\")\n", "plt.ylabel(\"Loss\")\n", "plt.grid(True)\n", "plt.tight_layout()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}